















































The Rise of Artificial Intelligence- Its Impact on Labor Market and Beyond 11_29_15 Final


	 1	

NOT FOR DISTRIBUTION 
 

The Rise of Artificial Intelligence: Its Impact on Labor Market and 
Beyond 

 
 

Robert Niewiadomski & Dennis Anderson 
	
	
“We know that blind evolutionary processes can produce human-level general intelligence, since they have already 
done so at least once. Evolutionary processes with foresight—that is, genetic programs designed and guided by an 
intelligent human programmer—should be able to achieve a similar outcome with far greater efficiency.”  

- Nick Bostrom, Superintelligence, Paths, Dangers, Strategies 
 
 

 
 

From Galatea and Golems to Future Syneoids 
 
The unique urge to create an artificial man has a long tradition since the 

antiquities, “Hephaestus, the master craftsman, grants a human voice (…) to his golden 
mechanical handmaidens (Gera, 2003, p. 114)” and Jewish folklore includes many 
versions of golems –anthropomorphic beings created from clay. Myths, religions, and 
popular culture displayed this obsession with varying intensity thorough history. Pamela 
McCorduck (1979), who investigated this phenomenon, concludes that as humans, we 
have been engaged in this peculiar form of self-reproduction by attempting to fulfill the 
urgent desire, bypassing the ordinary means, in order to recreate what is the essential to 
us since the dawn of Western intellectual history. Contemporary incarnation of this 
ancient desire can be observed in robotics. 

Currently, we are on the verge of an unprecedented technological revolution 
involving intelligent robots powered by artificial intelligence (AI).  It is important to note 
that what we are experiencing at this stage – Google self-driving cars, Apple Siri, Google 
Photo Search, robots exhibiting behavior similar to human- are examples of the “weak AI 
(WAI).” The concept asserts that machines could act as if they were intelligent. It is a sort 
of limited intelligence. To the contrary, “strong AI” (SAI) is the higher level of AI, often 
referred to as “artificial general intelligence” (AGI) –it entails the possibility of machines 
actually thinking (Russell & Norvig, 1995). According to this view SAI would possess 
the full range of human cognitive abilities. Since such AI, as predicted, would experience 
exponential growth, it would quite swiftly reach a level exceeding human capacities – a 
point often referred to as “singularity.” If we assume a physicalist position regarding the 
nature of human consciousness, we ought to also seriously consider the possibility that 
SAI would be, at some point, able to genuinely experience subjective mental states such 
as consciousness. The emergence of consciousness in this case could be an incremental 
phenomenon, similar to varying degrees of mental capacities in animals.  The 
terminology attempting to capture the phenomenon described above varies and it does not 
quite reflect its full nature and scope. For the purpose of our investigation we will refer to 



	 2	

it as the syneoid –a term derived from Greek συνειδητός (corresponding to English 
conscious) and the suffix –oid suggesting “likeness” or “form of” as well as bearing 
resemblance to android (human-like robot).   

Throughout history, technological advancements have impacted how we worked, 
lived and died.  The canonical narration pertaining to the impact of major technological 
leaps in civilization on employment, particularly the paradigmatic Industrial Revolution, 
follows the familiar pattern: they lead to substantial shifts in types of jobs, affecting 
perhaps large, but limited area of employment; certainly, total number of jobs was not 
affected and, if anything, technological revolutions expand economy and, ultimately, 
expand employment opportunity by opening new areas of exploration. Certainly, in a 
short term, this was a messy process, many lost taken away from them by efficient, 
laborsaving inventions, but, in the long run, through gradual adaptations, improved 
education and higher qualification, everyone benefited. Thus, in essence, technological 
revolution has been typically painted, in spite of its shortcomings and damages, as a 
disruptor, the true engine of human progress.    

The arrival of the syneoid, however, would have staggering consequences for the 
future of human employment and beyond. In this chapter, we intend to explore the impact 
of the rapid expansion of SAI in relations to the future labor market. We argue that this 
rather optimistic, even naïve scenario, collapses while confronted with the explosion of 
SAI, as none of the historical extrapolations will apply to the processes that are currently 
shaping up. The history might have had repeated itself with several technological 
revolutionary cycles, but, this time we will be confronted with a radical new, for the 
arrival of which we are utterly unprepared, and as we are about to see, in principle, we 
cannot.    

Interestingly, although the research on SAI is growing fast, it did not capture the 
interest of general public beyond some portrayals in science fiction. That does not 
translate, however, into a serious public debate on the speed and consequences of this 
technological revolution in statu nascendi.  Popular imagination seems to be captivated 
by other occurrences. The general public and both established and aspiring politicians are 
typically more concerned with, and not without a good reason, the issues of climate 
change, the rise of jihadist extremism, and ever-widening income gap. Politicians and 
experts turn to history looking for solutions, and while some of them proved to yield 
positive results in the past, none will work to alleviate the incoming next wave of 
unemployment due to automation.” for the causes of it are drastically different from 
anything from what we have experiences so far. 

 
Will History Repeat Itself? –Not this Time  

 
In the past, the slower pace of transformations allowed enough time to adopt and 

balance the labor supply and demand. By the dawn of 19th century, technological 
advancements decreased the agricultural labor in Britain from 75% in 1500 to 35%. The 
excess labor supply was shifted to manufacturing and commerce. This transfer to 
manufacturing first happened on a small scale, typically in small workshops or homes.  It 
is interesting to note that this was the beginning of wealth transfer from agriculture to 
industry. Most of them disappeared when the automation and steam engine brought about 
by the Industrial Revolution, which marked a dramatic change in process of production -



	 3	

division of labor; mechanization, specialized workforce. The driving force behind 
automation has been and still is the desire to improve the profit margin by cutting the 
workforce perceived as cumbersome and inefficient.  

The modern factory also created a new type of workers -precisely defined 
components in steam-driven machinery. As industrial instruments became more 
sophisticated, the demand for more skilled labor force increased. The low-skilled ones 
were pushed out yet again. The investment in education in the 19th century provided a 
supply of qualified labor, and this trend continued into the 20th century as post-secondary 
education became increasingly common. WWII fueled and accelerated the development 
of complex machines and systems as created the global supply chain management. After 
the war, workers in the West enjoyed unprecedented prosperity that provided them with a 
secure pathway to middle class. Even the initial introduction of computers had no 
immediate impact on job market. Obviously, mathematical calculations done by hand by 
numerous people became obsolete, but, in time, the invention greatly increased the 
productivity of others and created a plethora of computer-related jobs. Many other 
technical innovations had similar effects. New machinery displaced handicraft producers 
across numerous industries, from textiles to metalworking. 

 Naturally, the mere possibility of automation didn’t mean that it always made sense 
to implement it. Often, it was more profitable to outsource jobs and take advantage of 
cheap labor in developing counties. This was accelerated by globalization. Western 
workers found themselves struggling yet again, facing increasing competition from both 
machines and cheap overseas labor.  Since the 1950’s employment in manufacturing in 
the U.S. declined sharply due to cheap imports and rising cost of labor. It was inevitable 
that most available jobs moved to service industry. Eventually, we transitioned into a 
consumption-based economy where many jobs shifted to service industries selling goods 
and delivering services at a mass scale. However, service industry already displays 
similar techniques of maximization of output on the expense of labor force as we have 
seen in manufacturing.  

Eventually, the deterioration of employment opportunities expanded to highly skilled 
college educated workforce as well. As Martin Ford argues, AI ultimately will lead to 
evaporation of most blue and white collar jobs alike leading to even accelerating the 
process of middle-class shrinking that are already experiencing tremendous costs of 
health care and education. Ford scenarios predict massive unemployment, and a complete 
breakdown of consumer-based economy itself (Ford, 2009). 

WAI already led to the emergence of a radically different sort of technologies capable 
of performing tasks previously done by humans: quite sophisticates and speedy 
reasoning, judgment and perception. One would be perhaps tempted to describing theses 
capacities as constituting a sort of agency. As Jeremy Howard (2014) points out, 
“Computers right now can do the things that humans spend most of their time being paid 
to do, so now's the time to start thinking about how we're going to adjust our social 
structures and economic structures to be aware of this new reality.” This rapid automation 
of physical or mechanical or repetitive jobs by now has resulted in elimination of various 
occupations and the process only just started. The obvious question one ought to ask is 
how fast this will continue and cause eventually massive unemployment? Job market 
needs a balance of supply and demand but when human labor is no longer required, this 
balance no longer exists as humans are no longer part of the labor market. At this 



	 4	

historical junction, the fundamental question then is: Will AI reduce the need for human 
toil and, as a result, cause technological unemployment There are still two opposing main 
answers to this question as articulated by Nils Nilsson (1984, p. 5). 

“Some claim that AI is not really very different from other technologies 
that have supported automation and increased productivity (…) Like them, 
AI may also lead ultimately to an expanding economy with a concomitant 
expansion of employment opportunities. At worst, (…) there will be some, 
perhaps even substantial shifts in the types of jobs, but certainly no overall 
reduction in the total number of jobs. In my opinion, however, such an 
outcome is based on an overly conservative appraisal of the real potential 
of artificial intelligence. Others accept a rather strong hypothesis with 
regard to AI -one that sets AI far apart from previous labor-saving 
technologies. Quite simply, this hypothesis affirms that anything people 
can do, AI can do as well. Certainly AI has not yet achieved human-level 
performance in many important functions, but many AI scientists believe 
that artificial intelligence inevitably will equal and surpass human mental 
abilities.” 
 

The “optimistic scenario” as articulated above, is based on the naïve extrapolation of 
historical experience. In a nutshell, it basically claims that AI is no different from other 
technological advances we have witnessed in the past. It will, just like them, after all, 
lead to the expansion of employment opportunities in other fields. This shift might be 
quite unpleasant in a short run, the human cost conceivably high, but, in the end, people 
are pushed out from their jobs by automation will eventually find employment elsewhere. 
To believe otherwise –according to the optimists – is erroneous. Keynes (1931) wrote in 
his famous essay, about the causes of widespread technological unemployment occurring 
due to discoveries capable of economizing the use of labor that are surpassing the pace at 
which we can find new uses for labor. Nevertheless, he also concluded that technological 
unemployment is only, as he put it, temporary phase of maladjustment that will be solved 
in a long run. 

Those, who support this view, argue that those concerns pertaining to long–lasting or 
even permanent technological unemployment result from the “Luddite fallacy” chiefly 
because they fail to recognize compensation effects. The term was coined by drawing a 
comparison to the 19th century reasoning of the English textile workers, who were 
concerned with the machinery threatening their livelihood, introduced during the 
Industrial Revolution, and resorted to violent uprisings. They tend to believe that the 
technological progress will eventually lead to the increase of wealth in a given society as 
a whole, or that the advances, as mentioned above, will lead to creation of new 
employment opportunities. In other words, the “Luddite fallacy” proponents also tend to 
point out that the worries about long-lasting or permanent technological unemployment 
are unwarranted because there is no such thing as a finite amount of work available to us. 
Thus, we should not be concerned precisely because new opportunities will always 
emerge that only humans will be able to perform.   

On the surface this assumption might seem justified and intuitive since we tend to 
extrapolate from the past. Past is, however, not necessary a solid predictor of the future, 
particularly when we are confronted with a novel phenomenon. The believe in such 



	 5	

fallacy constituted a prevalent view of mainstream economists on the problem of 
technological unemployment until recently and many still subscribes to it.  

Nevertheless, some cracks in this reasoning begin are easy to spot. Assuming, for the 
sake of the argument, that the technological progress, in fact, will increase the aggregate 
wealth of the society, it is not obvious that the distribution of the wealth will be equitable. 
It goes without saying that distribution of wealth matters a great deal. For example, in a 
technocratic society, such as ours, an enormous amount of wealth is concentrated in the 
hands of 1%. This does not mean, however, that everyone else benefits through 
mechanisms of job-creation. We can see that in similar mechanism –the correlation 
between lowering taxes for the wealthiest and job -creation is highly arguable. Thus, 
supposed increase in overall wealth does not axiomatically translate into lower rate of 
unemployment –as enthusiasts of “trickle down” economy would like to believe.   

In addition, according to the optimistic narrative, long-lasting technological 
unemployment typically requires a massive effort to educate the population –again a 
lesson learned from the Industrial Revolution. In short, education is supposed to be the 
key to solving this issue. As Paul Krugman (2013, para. 4) puts it by revisiting the 
problems faced by, the previously ridiculed, 19th century textile workers:  

  
“Until recently, the conventional wisdom about the effects of technology 
on workers was, in a way, comforting. Clearly, many workers weren’t 
sharing fully — or, in many cases, at all — in the benefits of rising 
productivity; instead, the bulk of the gains were going to a minority of the 
work force. But this, the story went, was because modern technology was 
raising the demand for highly educated workers while reducing the 
demand for less educated workers. And the solution was more 
education.” 
 
Indeed, mostly free and compulsory public education emerged in the West as a 

result of ideas of the Enlightenment a feeding pipe for factories of the industrial age. This 
model of “keeping up “with the technology seemed to work fairly well for some portions 
of the population, at least until recently. Several factors are at play here: the outdated 
model of education that is simply too specialized and often trains provides training that 
quickly lacks relevance on the labor market. After all, it is difficult to blame institution of 
higher education for not predicting what skills will be desirable on the rapidly shifting 
labor market even in not-to-distant future. No one can really predict that. The second 
factor pertains to the massive democratization of higher education experienced virtually 
in all developed countries. This educational inflation, occasionally leading to outburst of 
frustration comparable to those of mentioned textile workers, effectively cancels out any 
competitive edge previously enjoyed by college degree holders in the past, 

  
“Today (…) a much darker picture of the effects of technology on labor is 
emerging. In this picture, highly educated workers are as likely as less 
educated workers to find themselves displaced and devalued, and pushing 
for more education may create as many problems as it solves. (…) So 
should workers simply be prepared to acquire new skills? The 
woolworkers of 18th-century Leeds addressed this issue back in 1786: 



	 6	

“Who will maintain our families, whilst we undertake the arduous task” 
of learning a new trade? Also, they asked, what will happen if the new 
trade, in turn, gets devalued by further technological advance? 
And the modern counterparts of those woolworkers might well ask 
further, what will happen to us if, like so many students, we go deep into 
debt to acquire the skills we’re told we need, only to learn that the 
economy no longer wants those skills? 
Education, then, is no longer the answer to rising inequality, if it ever 
was (Krugman, 2013, para. 6).” 
 

Putting aside the question of what kind of training and qualifications would be needed 
to access this emerging employment opportunities and how many individual would have 
an actual chance to do so etc., there are more serious problems with this line of reasoning. 
Even well adjusted to the job market and flexible individuals might not be able to keep up 
with the speed of technological acceleration. This is how Krugman (2013, para. 1-2) 
explain this phenomenon:  

 
“In 1786, the cloth workers of Leeds, a wool-industry center in northern 
England, issued a protest against the growing use of “scribbling” 
machines, which were taking over a task formerly performed by skilled 
labor. “How are those men, thus thrown out of employ to provide for 
their families?” (…) Those weren’t foolish questions. Mechanization 
eventually — that is, after a couple of generations — led to a broad rise 
in British living standards. But it’s far from clear whether typical 
workers reaped any benefits during the early stages of the Industrial 
Revolution; many workers were clearly hurt. And often the workers hurt 
most were those who had, with effort, acquired valuable skills — only to 
find those skills suddenly devalued.” 

 
More fundamentally, the crucial point that get lost in the current debates, is that 

even if some number of very highly specialized occupations will be created, with the 
arrival of syneoids, “this kind of work can also be performed by AI devices without 
necessarily implying more jobs for humans (Nilsson, Ibid.).”  

The above set of issues gradually begins to gain traction in the high strata of the 
more forward-thinking individuals. According to the posted agenda of the Davos World 
Economic Forum in 2016 the incoming shift, framed as the Fourth Industrial Revolution, 
is recognized a paradigmatic change that will affect virtually every sphere of human life. 
To the question why this revolution would be any different from the previous ones we 
experienced, Klaus Schwab delivers the answer with blunt honesty, 

“The first industrial revolution began in the latter half of the 18th 
century, and brought us mechanical production through the steam 
engine, cotton spinning and then railroads. The second industrial 
revolution took place around the turn of the 20th century and brought 
mass production through assembly lines and electrification. The third 
industrial revolution was the computer revolution, starting in the 1960s 



	 7	

that brought us the mainframe and then personal computing, as well as 
the Internet. Today we are in the midst of the Fourth Industrial 
Revolution, which will affect governments, businesses and economies in 
very substantial ways. We should not underestimate the change ahead 
of us, as there are at least three differences between this revolution and 
the previous ones (Schwab, 2015, para. 4-5).” 

 Schwab articulates three major components that differentiate this new 
fundamental change from previous technological revolutions. In a nutshell, the first one 
has to do with its speed. The current change is happening with an unprecedented 
acceleration, as opposed to the previous ones that took decades or more to fully unfold. 
Secondly, the Fourth Industrial Revolution is not limited only to one area. In incorporates 
such elements as mobile networks, nanotechnology, and brain research, 3D printing, 
materials science, computing, networks “the interplay between all of these. (…) In 
addition, the accessibility and affordability of complex technologies will spread them 
farther and faster (Ibid. para. 6-8).” The third difference, according to Schwab, pertains 
the innovation of the entire system, as opposed to one product or area. The phenomenon 
of “sharing economy” is only one of many manifestations of that aspect.  

As mentioned, the impact of this paradigmatic shift will be profound, and will 
affect the business model, the ways governments operate, and last but not least, the future 
usefulness of skills and labor. As Fon Mathuros (2015, para. 2) points out, 
   

“Earlier Industrial Revolutions advanced human progress through 
new forms of power generation, mass production and information 
processing. Building on a ubiquitous and mobile internet, smaller, 
cheaper and more powerful sensors, as well as artificial intelligence 
and machine learning, the Fourth Industrial Revolution is distinct in 
the speed, scale and force at which it transforms entire systems of 
production, distribution, consumption – and possibly the very essence 
of human nature.” 
 

The Davos Forum organizers, although recognizing the magnitude of 
imminent shift, tend to portray the Fourth Industrial Revolution in cautiously 
optimistic terms as perhaps one of the greatest challenges we will encounter, and 
yet, a possible opportunity. Nevertheless, they cannot disregard that two aspects of 
human life will be affected most significantly by the future changes –the value and 
significance of human work and profound changes in social structures.  

 
The New Cambrian Explosion  
 

 In a peculiar way, some aspects of the development of robotics mimic the natural 
processes of evolution. Gill Pratt (2015, p. 51) draws an intriguing comparison between 
the two. 

“About half a billion years ago, life on earth experienced a short 
period of very rapid diversification called the “Cambrian Explosion.” 
Many theories have been proposed for the cause of the Cambrian 



	 8	

Explosion, with one of the most provocative being the evolution of 
vision, which allowed animals to dramatically increase their ability to 
hunt and find mates (…) Today, technological developments on several 
fronts are fomenting a similar explosion in the diversification and 
applicability of robotics. Many of the base hardware technologies on 
which robots depend—particularly computing, data storage, and 
communications—have been improving at exponential growth rates. 
Two newly blossoming technologies—“Cloud Robotics” and “Deep 
Learning”—could leverage these base technologies in a virtuous cycle 
of explosive growth. In Cloud Robotics (…) every robot learns from the 
experiences of all robots, which leads to rapid growth of robot 
competence, particularly as the number of robots grows. Deep 
Learning algorithms are a method for robots to learn and generalize 
their associations based on very large (and often cloud-based) 
“training sets” that typically include millions of examples. (…) One of 
the robotic capabilities recently enabled by these combined 
technologies is vision—the same capability that may have played a 
leading role in the Cambrian Explosion.” 

 
Initially, AI and robotics were separate fields, -computer science and engineering, 

respectively. Currently it is the precisely the combination of the two -the robotic 
embodiment of AI- that sets the trend for the future. Particularly, Deep Learning (DL), 
which is an algorithm-based method of “teaching a machine” how to learn from a 
massive sample sets, is one on the major mechanism driving forces. Current applications 
of DL already include automatic speech and image recognition, natural language 
processing, transforming images into description, discovery of drugs and toxicity, 
medical diagnostics, among others. DL certainly has the potential to become a stepping-
stone to creating syneoids, which through exponential growth would rapidly exceed any 
human faculties to a degree unimaginable. Ray Kurzweil, who shares Searle’s view that 
an appropriately programmed computer is a mind, makes a prediction that the equivalent 
capacity of one human brain will be available on desktop computers by 2020. Moreover, 
when AI starts to outstrip the collective total of all human intelligence, Kurzweil argues, 
humanity will enter, what he calls, the Singularity, marking a point at which change is so 
radical that it is no longer predictable (as cited in Wallach, p. 57). 

The kinds of operations that WAI is capable of performing now cannot be called 
thinking in the traditional sense, if what we mean by thinking, by definition, only takes 
place inside the human cranium (McCorduck, 2004) Nevertheless, many consider 
“making difficult judgments, the kind usually left to experts, choosing among plausible 
alternatives, and acting on those choices (Ibid.)” a form of thinking. McCorduck further 
elaborates, “Along with most people in AI, I consider what artificial intelligences do as a 
form of thinking, though I agree that these programs don't think just like human beings 
do, for the most part. I'm not sure that's even desirable. Why would we want AIs if all we 
want is human-level intelligence? There are plenty of humans on the planet. The field's 
big project is to make intelligences that exceed our own. As these programs come into 
our lives in more ways, we'll need programs that can explain their reasoning to us before 
we accept their decisions (Ibid.).” 



	 9	

Certainly, it is fair to say that AI and robotics have not crossed yet the level of 
singularity and thus reached the level of syneoids. However, this could be just a matter of 
time. Once they reach that stage, all jobs can be automated including key decisions. No 
human will be needed for manufacturing, services, knowledge, or even creative 
processes. Thus, “Luddite fallacy”, described before, may not be, in the end, a fallacy at 
all. This time around we simply might not be able to “catch up” precisely because human 
gradual learning cannot possibly be a competition to exponential growth of Deep 
Learning as Jeremy Howard (2014) explains,  

“80 percent of the world's employment in the developed world is stuff 
that computers have just learned how to do. What does that mean? 
Well, it'll be fine. They'll be replaced by other jobs. For example, there 
will be more jobs for data scientists. Well, not really. It doesn't take 
data scientists very long to build these things. For example, these four 
algorithms were all built by the same guy. So if you think, oh, it's all 
happened before, we've seen the results in the past of when new things 
come along and they get replaced by new jobs, what are these new jobs 
going to be? It's very hard for us to estimate this, because human 
performance grows at this gradual rate, but we now have a system, 
deep learning, that we know actually grows in capability 
exponentially.” 

 
In light of the inevitable systemic technological unemployment, experts are 

constantly considering different solutions ranging from quite unrealistic ideas of banning 
innovation to various forms of wealth redistribution. It is not our intention here to 
examine the feasibility of these proposals. The reasons for that choice are quite simple: 
stopping the entire process is practically impossible, and once syneoids arrive, humans 
won’t make any decisions regarding that matter alone, as we will have to work it out 
with, far more intelligent and capable than us, “others.”  They would be able to colonize 
places inaccessible to human because of our limitations such us the need of oxygen, 
water, and food. 

Even though, we are not there yet, the loss of jobs due to the already available 
WAI will be staggering in a relatively short time. A very conservative study conducted at 
Oxford University by Frey and Osborne (2013) examined the effects of computerization 
alone on 702 detailed occupations. The authors used a Gaussian process classifier and 
determined that “about 47 percent of total U.S. employment is at risk (Ibid. p.1)” of 
computerization in a decade or two. As they point out, “The impact of computerisation on 
labour market outcomes is well-established in the literature, documenting the decline of 
employment in routine intensive occupations – i.e. occupations mainly consisting of tasks 
following well-defined procedures that can easily be performed by sophisticated 
algorithms. (…) Following recent technological advances, however, computerisation is 
now spreading to domains commonly defined as non-routine (Ibid. pp.2-15).” Frey & 
Osborne further argue that even though Machine Learning (ML) and Mobile Robotics 
(MR) enable computerization across large areas of non-routine tasks, there exists some 
“engendering bottlenecks” inhabiting total de-humanization of labor.  
 



	 10	

“Beyond these bottlenecks, however, we argue that it is largely already 
technologically possible to automate almost any task, provided that 
sufficient amounts of data are gathered for pattern recognition. Our 
model thus predicts that the pace at which these bottlenecks can be 
overcome will determine the extent of computerisation in the twenty-
first century. Hence, in short, while the task model predicts that 
computers for labour substitution will be confined to routine tasks, our 
model predicts that computerisation can be extended to any non-routine 
task that is not subject to any engineering bottlenecks to 
computerisation. These bottlenecks thus set the boundaries for the 
computerisation of non-routine tasks (Ibid. p.23).” 
 

  They identify three major “bottlenecks.” The first one perception and 
manipulation and, in essence pertains to the abilities to make precisely coordinated 
movements. This “bottleneck” seems to pose the least of the problem for computerization 
and robotization, since human body itself can be seen as a biological machine that had a 
very long to evolve and adapt to perform very complex of manipulating the environment. 
In principle, however, there is nothing, besides technological and perhaps financial 
limitations, that would prevent from replicating these abilities. The authors admit that, 
“with incremental technological improvements, the comparative advantage of human 
labour in perception and manipulation tasks could eventually diminish (Ibid. p. 39).” The 
second “bottleneck” indeed poses a serious challenge for AI at this stage. It has to do 
with creative intelligence –faculties we typically associate with solving unusual 
problems, artistic production etc. The third one, social intelligence, pertains to the ability 
of being aware of other’s reactions and understanding why they reacts in a given way; the 
ability to persuade others, and, finally, with a broadly understood, emotional support 
(Ibid. p. 31).  

This last obstacle is indeed unlikely to be solved by WAI at its present stage. 
Thus, the study reports that, “generalist occupations requiring knowledge of human 
heuristics, and specialist occupations involving the development of novel ideas and 
artifacts, are the least susceptible to computerization (Ibid. p. 40).” Based on these 
technological “bottlenecks”, their model assigns even specific probabilities that 
computerization will lead to job looses within about the next two decades. And, so they 
estimate that that probability would be 0.99 for telemarketers, 0.55 for commercial pilots, 
and only 0.003 for recreational therapists, presumably because the latter one would 
require technological overcoming of the social intelligence “bottleneck.”   

It is important to keep in mind, that Fry & Osborne’s analysis does rely on the 
technological  “bottlenecks” as they present themselves today. The history of 
technological advances, however, can be largely perceived as a struggle, astonishingly 
successful, of overcoming such “bottlenecks.” Thus, there is no reason to think that these 
current, undoubtedly serious “bottlenecks,” will become obsolete once the technology 
moves beyond the stage of WAI towards syneoids.  

Again, skeptics find it difficult to accept that syneoids could completely fill every 
available niche in the labor market and make human skills virtually obsolete. They 
believe that there is something uniquely special about human capacities, whether be 
intellectual or emotional that there will be always a margin of tasks syneoids simply 



	 11	

won’t be able to do, or people would prefer them being done by other humans. Those 
occupations would require human sensitivity that can satisfy deep interpersonal needs. 
This would involve such activities as making decisions in a court of law because, 
supposedly, we would prefer a person to by accountable for such decisions. The same 
would apply to CEO’s, generals, military and government leaders at every level.   
This could be the case if syneoids would exceed human capacities only intellectually, but 
as we stipulated, they would also become decision making fully autonomous agents, with 
whom we would have engage in interactions involving making moral choices. 

Schwab (2015, para. 13-15) gestures towards this issue of syneoids and humans 
living side by side by sketching two compelling scenarios, 

 “Very importantly, will the Fourth Industrial Revolution have a human heart and soul? 
Again, imagine living in the future, when robots and humans live side by side. Who would 
you trust, human or robot? Concretely: 

• If you were told you had a life-threatening illness and a human doctor 
prescribed treatment regime A, while an artificially intelligent robot 
prescribed treatment regime B, which treatment regime would you follow? 

• If you were falsely accused of a crime, would you rather be tried by a human 
judge or by an AI judge? ” 

It is obvious that Schwab scratches just the surface of the possible issues of the 
moral aspect of interactions with syneoids. For some this very prospect of such 
interactions and the moral uncertainty connected to it are frightening ideas. 
Perhaps, as some prominent figures such as Stephen Hawking and Elon Musk, 
argue, it would be better for us to stop the expansion of AI altogether, particularly 
with respect to autonomous warfare agents (Musk et al.).  Some postulate that our 
efforts would be concentrated on designing these agents in such way “so that they 
honor the broader set of values and laws humans demand of human moral 
(Wallach, p. 6).” 

However, at this point, it is far too late to ask the question whether the humanity 
need intelligent machines making moral decisions. Machines are already making 
decisions with moral consequences. As mentioned, machines, whether intelligent or not, 
even tools, had moral impact on our lives and, in fact, always had. James Moor proposed 
a hierarchical model that categorizes different levels of machine’s ethical agency. At the 
very bottom are, what he refers to “ethical impact agents.” This category includes “any 
machine that can be evaluated for its ethical consequences (as cited in Wallach, p. 33-
34).” The next level Moore calls “implicit ethical agents” – “machines whose designers 
have made an effort to design them so that they don’t have negative ethical effects, by 
addressing safety and critical reliability concerns during the design process (Ibid.).” 
“Explicit ethical agents” occupy the third level of Moor’s hierarchy. These are machines 
that can reason about ethics using ethical categories as part of their internal programming. 
Setting proper ramifications for the design of such  “implicit ethical agents” and 
establishing an appropriate ethical code for their decision-making is currently the main 
concern of the field of computer ethics. Everything that lies beyond these three levels 



	 12	

Moor considers “full ethical agents” –machines that can make “explicit moral judgments 
and are generally quite competent in justifying such decisions. This level of performance 
is often presumed to require a capacity for consciousness, intentionality, and free will. If 
any of these three is lacking in a human context, then the person’s moral agency and legal 
culpability comes into question (Ibid).” This highest level of full ethical agents matches 
our definition of a syneoid.  Naturally, it would be quite naïve to think that syneoids 
would necessarily share our moral standards and sensitivities. Syneoid’s morality could 
go much further beyond anything we can envision based on our anthropological 
imagination and knowledge of varying moral standards in human cultures; it could be a 
set of norms radically alien in general -based on different preference and goals.  

 
 

Why consciousness matters? 
 
Some philosophers and scientists doubt the very possibility of artificial “full 

ethical agents” precisely because the idea of them possessing consciousness, 
intentionality, and free will seem uniquely human. From all three –consciousness is 
probably considered to be at the core of human personhood, which is the only point of 
reference of personhood we have available. In order to understand why consciousness 
matters in the context of syneoids, we should go back to the18th century materialist 
philosophy. In reaction to the previous substance dualism, La Mettrie, in his L'homme 
Machine, extended to human beings Descartes’ argument that animals are automatons. 
Karl Popper  (1978, pp. 224-225) was convinced that La Mettrie’s idea gained a greater 
reinforcement by the theory of evolution.  

“ […] The doctrine that man is a machine was argued most 
forcefully and seriously in 1751, long before the theory of evolution 
became generally accepted, by de Lamettrie; and the theory of 
evolution gave the problem an even sharper edge, by suggesting that 
there may be no clear distinction between living matter and dead 
matter.” 

 
In principle, the emergence of conscious syneoid is plausible provided that few 

premises that we would need to commit to are true. One of them is an ontological 
assumption, rooted in neuroscience, concerning the nature of consciousness. The brain 
itself –as the argument runs- is a naturally occurring material object that emerged through 
evolutionary processes and as such, at least in principle, is replicable just as are with 
sufficiently advanced technology. Secondly, mental states are essentially brain states. 
Thus, consciousness supervenes on the brain and, just as the brain it is a natural 
phenomenon that is a product of a very long incremental evolutionary process.  Thirdly, 
we have to commit to yet another premise that neurons are not the only material that can 
create a mental state (Dennett, 1991). Therefore, it is just a mater of time that we, as 
conscious beings, will be able to imitate it by some means that might be not necessarily 
be biological in nature.  

This argument is further supported by “Artificial Life” projects that essentially 
see biology as a naturally occurring information technology. That “technology” could be 
reversed engineered and synthesized in other kinds of technologies, again, not necessarily 



	 13	

biological ones (Sullins, 2014). Consciousness would, therefore, be a product of this 
naturally occurring information technology. It is important to make another observation –
consciousness, after all, is present, to varying degrees, in different species. In short, if we 
as humans are naturally occurring robots, and just so happened we possess consciousness, 
there is nothing, in principle, preventing replication of this natural phenomenon 
artificially.  

There is obviously the question of how would we even know that we have created 
a conscious machine, or that a conscious machine emerged though, for example, 
processes similar to Deep Learning? The famous Turing Test cannot answer this question 
as demonstrated by Searle in his “Chinese room” thought experiment. One might even 
argue that perhaps a consciousness test, in principle, cannot be done if we set the testing 
standard unreasonably high. After all, how can we be absolutely sure that other human 
fellows have mental states or just behave as if they have them? Such high requirements 
might lead us towards metaphysical solipsism – a doctrine with very vague practical 
consequences, and as such not very useful in our investigation.  

One can, obviously stipulate that consciousness is a something that occurs in 
human brain alone, and then, by analogy, for example, make inferences that other 
humans possess that property as well. It seems fair to say that most people use this 
analogy, or symmetry principle in their common sense approach to the issue of 
consciousness of others. This can be further supported by MRI imaging showing 
correlation between changes in brain activity with reports of subjective mental states of 
the subject. In fact, similar methods can be used to investigate the mental states of 
individuals who are unable to communicate in any way. These are obviously well 
established and perfectly sound scientific practices, and yet strictly speaking, they can 
only provide indirect proofs that others possess consciousness. Another commonsensical 
approach to solve the problem is based on common ancestry –the offspring, as a matter of 
rule, should display capabilities similar to those of those of its ancestors. This line of 
reasoning is just another way of making inferences by analogy. However, it is easy to 
spot a weak point of these common sense based reasoning. Provided that consciousness 
develops artificially and will neither brain-based, not biologically related to us, none of 
these common sense reasoning by analogy will apply. In case, there will be no way to 
discern if the intelligent machine really possesses consciousness, or is simulating it, one 
can obviously stick to the believe that only humans have it by definition, but that would 
be terribly arbitrary, and ultimately unjustified. Ultimately, if syneoids displayed mental 
and cognitive capabilities that are indiscernible from those displayed by humans that we 
“know” that they possess consciousness, it would be unclear, to say at least, on what 
basis we could conclude that syneoids do not have consciousness after all.   

The more fundamental question is obviously: Why would it even matter to know 
whether syneoids possess consciousness or they do not? With regards to the issue of 
technological unemployment the answer is – it matters. One could obviously argue that 
such question is superfluous since super intelligent “zombies” could replace us easily, 
and the issue whether they are conscious has nothing to do with that. However, going 
back to the previous issue that people might prefer for certain services or occupations to 
be performed by human persons only precisely because they are conscious beings, it 
becomes clear how artificially generated consciousness would eliminate this need as well. 
Thus, there would be virtually nothing left that “only humans could do.”   



	 14	

 The answer to the consciousness question also becomes extremely relevant when 
we consider the issue of interaction with syneoids. Again, one can dodge the bullet and 
insist that although we are dealing with beings displaying intelligence similar or superior 
to ours, nevertheless, they are qualitatively infinitely different; they don’t have some sort 
of “essence” that we have and, as such, they should be excluded from our moral 
considerations. Needless to say, there would be something incredibly odd in the argument 
that a syneoid capable of performing human functions, or even exceeding them -
intellectual, cognitive, and emotional is not treated as a person.  

Obviously, it goes without saying, that the notion of personhood is an incredibly 
complex one. Nevertheless, the most important components of the notion that are crucial 
for the purpose of the argument are: consciousness, agency, self-awareness, and 
possession of rights and duties (Taylor, 1985). It is also important to realize that 
personhood could be a matter of degree; could be gained and lost incrementally, and that 
does not need to be limited to humans. The latter statement has remarkable consequence, 
namely, being a member of the human species is nether necessary, nor sufficient to be, at 
lest to some degree, a person. It is currently commonly accepted that at least primates and 
some other mammal’s posses degree of personhood far beyond a rudimentary one, that 
qualifies then to be included in the circle of our moral considerations. Since the Oxford 
Group (Finsen & Finsen, 2009) the movement of animal rights gained traction in 
mainstream intellectual community. It drew its conclusions from Darwinism and 
consequentialism, and essentially stated that there is no mysterious essential difference 
between biologically speaking. Thus, the same type of moral continuum should be 
applied to other beings. It would be expected that a syneoid, conscious by definition, 
would realize its own personhood.  Thus, if someone or something is considered to be a 
person, we ought to have moral obligations towards it. The fact of its origin, the fact that 
it is neither a human person, nor a biological being is utterly irrelevant. It would be 
preposterous to deny such syneoids moral consideration based on the claim that it does 
not belong to human species.  In fact, it could be argued that such attitude is similar to 
speciesism –a type of prejudice such as sexism or racism.  

Moreover, it could be argues that, as along as a syneoid behaves as a person, it is 
actually irrelevant if it, factually, possesses consciousness the way we conceive it at all in 
order for us to have moral obligations towards it. After all, many of us commit to the 
notion that we have moral considerations for certain animals even if, for what we know, 
they do not posses consciousness, or display it to a certain, typically lesser than ours 
degree. The consequences of committing to the notion that syneoids should be included 
in the area of our moral concern are staggering. It would be incomprehensible that we 
could treat such AI as merely our tools, extension of our bodies, something that we can 
use, control, and manipulate or any possibility of the master-slave relationship. We would 
have no choice but to to concede that they are entitled to all rights that we reserve for 
persons. We would have to concede that they have freedom of choice according to their 
preferences and needs, etc. the notion of our “uniqueness” would evaporate in an instant.   

  
 

Co-existence, Competition, Convergence 
 



	 15	

Job, occupation, profession, career –they are not merely sources of economic 
sustenance; it is far more than a mere necessity or burden. In many cases, it defines who 
we are.  Work, whether paid or pursued for passion, whether fairly simple or highly 
sophisticated, provides fulfillment, sense of community, exploration, discovery, an 
invention. Professionalism, craftsmanship, refinement, and artistry have been truly the 
driving forces of our civilization; for many –a fulfillment of an existential void, a deep 
sense of participation in a process of creation, mastery over nature, instrument of control, 
and, ultimately, power.  

Our inventions defined the work we engaged in for centuries from melding to typing; 
created new industries and employment opportunities around them. These inventions, 
intending to control the environment, had often unforeseen consequences that reshaped 
the way we lived, interacted with each other, and redefined our societal rules. To echo the 
famous maxim attributed to McLuhan that encapsulates this phenomenon: “We shape our 
tools and thereafter our tools shape us.” Occasionally, an emergence of new technological 
advances such as, domestication of animals, creation of printing press, or invention of 
computers had a paradigm-changing effect leading to major disintegration in the status 
quo, shifts and disruptions in the existing power structures. So, we were adapting, not 
without struggle, failures, and suffering, to these changes. The unstoppable progress of 
AI, however, and the eventual arrival of syneoids will confront us with the challenge to 
radically re-negotiate norms of coexistence with them without any guarantee of success.  

In the context of a workplace, but not limited to it, possibly the best-case scenario 
would involve a collaborative co-existence. But it might likely turn into a combative 
competition over resources and power –something we are quite familiar with already. 
The Darwinian principle of the survival of the fittest could very well swing the odds to 
the advantage of syneoids. The end result of such competition could very well be the end 
of our kind, or perhaps, on the more optimistic note, as Buchannan argues: “humans will 
go the way bacteria did. They existed before the Cambrian explosion, and they still exist 
today. In fact, they continue to thrive. They just don't hold the dominant position they 
once did (Buchannan, 2015).” This perceived disadvantage of our species prompted some 
to entertaining the idea that we ought to use the ever-expanding information technology 
to transform ourselves into a new post-human species, as a part of adaptive response to 
environmental pressures. Preserving the natural state of a human body -claim the 
proponents of transhumanism- is an obstacle to progress in the self-directed evolution 
(Bostrom, 2005).  

The arrival of syneoids will confront us on so many levels with the type of radical 
“other” - so different from us and yet a former “tool” “created in our image.” It will 
launch a journey with the “other” without precedence, unpredictable, unforeseeable, and 
volatile. We will be faces with many questions and to echo Pamela McCorduck (2004), 
that, “at that point at that point, we turn to our own smart machines for advice on what 
the best next move is for the human race.” And just like the well being of most species 
currently existing is greatly affected by our decisions and action, so we might find 
ourselves in a similar position upon the arrival of syneoids.  
 
 

 
 



	 16	

 
References 

 
Bostrom, N. (2005). "A history of transhumanist thought" (PDF). Journal of 
Evolution and Technology. Retrieved October 13, 2015. 
 
Bostrom, N. (2014). Superintelligence: Paths, dangers, strategies. Oxford : Oxford 
University Press. 
 
Buchanan, M. (2015, August 13). Get Ready for the Robot Explosion. Retrieved 
November 13, 2015, from http://www.bloombergview.com/articles/2015-08-13/get-
ready-for-the-robot-explosion 
 
Davis, N. (2015, November 10). 5 ways of understanding the Fourth Industrial 
Revolution. Retrieved November 12, 2015, from 
https://agenda.weforum.org/2015/11/5-ways-of-understanding-the-fourth-industrial-
revolution/ 
 
Finsen, S. & Finsen, L. (2009) "Animal rights movement," in Marc Bekoff (ed.). The 
Encyclopedia of Animal Rights and Animal Welfare. Greenwood. 
 
Daniel Dennett (1991). Chapter 14. Consciousness Imagined. Consciousness 
Explained. Back Bay Books. pp. 431–455. 
 
Ford, M. (2009). The lights in the tunnel: Automation, accelerating technology and 
the economy of the future. U.S.: Acculant Publishing. 
 
Frey, C., & Osborne, M. (2013, September 17). The Future of Employment: How 
Susceptible are Jobs to Computerisation? Retrieved from 
http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment
.pdf 
 
Gera, D. (2003). Ancient Greek ideas on speech, language, and civilization. Oxford: 
Oxford University Press. 
 
Howard, J. (2014, December). The wonderful and terrifying implications of 
computers that can learn. Lecture presented at TED. 
 
Keynes, J.M. (1931). Economic possibilities for our grandchildren (1930). Essays in 
persuasion, pp. 358–73. 
 
Krugman, P. (2013, June 13). Sympathy for the Luddites. Retrieved November 13, 
2015, from http://www.nytimes.com/2013/06/14/opinion/krugman-sympathy-for-the-
luddites.html 
 



	 17	

Musk, Wozniak and Hawking urge ban on warfare AI and autonomous weapons. 
(n.d.). Retrieved November 5, 2015, from http://www.msn.com/en-
gb/news/other/musk-wozniak-and-hawking-urge-ban-on-warfare-ai-and-autonomous-
weapons/ar-AAdxy1e 
 
Mathuros, F. (2015, November 10). What is the theme of Davos 2016? Retrieved 
November 12, 2015, from https://agenda.weforum.org/2015/11/what-is-the-theme-of-
davos-2016/ 
 
 
McCorduck, P. (1979). Machines who think: A personal inquiry into the history and 
prospects of artificial intelligence. San Francisco: W.H. Freeman. 
 
McCorduck, P. (2004). FAQ answered by Pamela McCorduck. Retrieved November 
7, 2015, from http://www.pamelamc.com/html/machines_who_think.html 
 
Nilsson, N. (1984). Artificial Intelligence, Employment, and Income. AI Magazine, 
5-14. 
 
 
Popper, K. (1973). Of Clouds and Clocks. In Objective Knowledge (Corrected ed., p. 
206–55). Oxford: Oxford University Press. 
 
Pratt, G. (2015). Is a Cambrian Explosion Coming for Robotics? Journal of Economic 
Perspectives, 51-60. 
 
Russell, S., & Norvig, P. (1995). Artificial intelligence: A modern approach. 
Englewood Cliffs, N.J.: Prentice Hall. 
 
Schwab, K. (2015, October 27). Will the Fourth Industrial Revolution have a human 
heart? Retrieved November 12, 2015, from https://agenda.weforum.org/2015/10/will-
the-fourth-industrial-revolution-have-a-human-heart-and-soul/ 
 
Sullins, J. (2014) "Information Technology and Moral Values", The Stanford 
Encyclopedia of Philosophy, Edward N. Zalta (ed.), URL = 
<http://plato.stanford.edu/archives/spr2014/entries/it-moral-values/>. 
 
Taylor, C. (1985). The concept of a person. Philosophical Papers Human Agency and 
Language, Volume 1, 97-114. 
 
Wallach, W. (2009). Moral machines: Teaching robots right from wrong. Oxford: 
Oxford University Press.    

	


