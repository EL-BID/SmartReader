I. Introduction That computers are ubiquitous in contemporary life is self-evident.
The share of information processing equipment and software in private, nonresidential investment rose from approximately 8 percent to more than 30 percent between 1950 and 2012, with the largest leap occurring between 1990 and 2000.1 It is hard to think of a prior historical episode where a single category of capital investment came so rapidly to dominate all others, now accounting for close to one in three business investment dollars.2 Given their ubiquity, it is tempting to infer that there is no task to which computers are not suited.
But that leap of logic is unfounded. Human tasks that have proved most amenable to computerization are those that follow explicit, codifiable procedures—such as multiplication—where computers now vastly exceed human labor in speed, quality, accuracy and cost efficiency.3 Tasks that have proved most vexing to automate are those that demand flexibility, judgment and common sense—skills that we understand only tacitly—for example, developing a hypothesis or organizing a closet.
In these tasks, computers are often less sophisticated than preschool-age children. The interplay between machine and human comparative advantage allows computers to substitute for workers in performing routine, codifiable tasks while amplifying the comparative advantage of workers in supplying problem-solving skills, adaptability and creativity.
Understanding this interplay is central to interpreting and forecasting the changing structure of employment in the U.S. and other industrialized countries.
This understanding is also is at the heart of the increasingly prominent debate about whether the rapid pace of automation threatens to render the demand for human labor obsolete over the next several decades. This paper offers a conceptual and empirical overview of the evolving relationship between computer capability and human skill demands. I begin by sketching the historical thinking about machine displacement of human labor, and then consider the contemporary incarnation of this displacement—labor market polarization, meaning the simultaneous growth of high-education, high-wage and low-education, low-wages jobs—a manifestation of Polanyi’s paradox.
I discuss both the explanatory power of the polarization phenomenon and some key puzzles that confront it. I finally reflect on how recent advances in artificial intelligence and robotics should shape our thinking about the likely trajectory of occupational change and employment growth. A key observation of the paper is that journalists and expert commentators overstate the extent of machine substitution for human labor and ignore the strong complementarities that increase productivity, raise earnings and augment demand for skilled labor.
The challenges to substituting machines for workers in tasks requiring flexibility, judgment and common sense remain immense. Contemporary computer science seeks to overcome Polanyi’s paradox by building machines that learn from human examples, thus inferring the rules that we tacitly apply but do not explicitly understand. II.
A Brief History of Automation Anxiety Anxiety about the adverse effects of technological change on employment has a venerable history.4 In the early 19th century, a group of English textile artisans calling themselves the Luddites staged a machine-trashing rebellion in protest of the rapid automation of textile production, which they feared jeopardized their livelihoods. Their actions earned the term Luddite an (unflattering) entry in the popular lexicon.
Economists have historically rejected the concerns of the Luddites as an example of the “lump of labor” fallacy, the supposition that an increase in labor productivity inevitably reduces employment because there is only a finite amount of work to do. While intuitively appealing, the notion that productivity gains reduce employment has received little historical support. The employment-to-population ratio, for example, rose over the course of the 20th century as women moved from home to market, and the unemployment rate fluctuated cyclically, with no long-term increase.
Yet, despite sustained increases in material standards of living, fear of the adverse employment consequences of technological advancement has recurred repeatedly in the 20th century. In his widely discussed Depression-era essay “Economic Possibilities for our Grandchildren,” John Maynard Keynes (1930) foresaw that in a century’s time, “we may be able to perform all the operations of agriculture, mining, and manufacture with a quarter of the human effort to which we have been accustomed.” Keynes viewed these developments as posing short-term challenges, “For the moment the very rapidity of these changes is hurting us and bringing difficult problems to solve.
… We are being afflicted with a new disease of which some readers may not yet have heard the name, but of which they will hear a great deal in the years to come—namely, technological unemployment.” But Keynes was sanguine about the long run, opining that “this is only a temporary phase of maladjustment,” and predicting that the 15-hour workweek (supporting a high standard of living) would be commonplace in a century’s time. Keynes’ projection that the maladjustment was “temporary” was a bold one given that he was writing during the Great Depression.
But the end of the Second World War seemed to affirm the rising prosperity that Keynes had foreseen. Perhaps more surprising is that “automation anxiety” recurred two decades after the Second World War, during what was arguably the height of American economic pre-eminence.
In 1964, President Johnson empaneled a “blue ribbon” National Commission on Technology, Automation, and Economic Progress, whose charge was “to identify and assess the past effects and the current and prospective role and pace of technological change; to identify and describe the impact of technological and economic change on production and employment, including new job requirements and the major types of worker displacement, both technologically and economic, which are likely to occur during the next 10 years.” While the commission ultimately concluded that automation did not threaten employment at that time, it recommended as insurance against this possibility, “a guaranteed minimum income for each family; using the government as the employer of last resort for the hard core jobless; two years of free education in either community or vocational colleges; a fully administered federal employment service, and individual Federal Reserve Bank sponsorship in area economic development free from the Fed’s national headquarters” (The Herald Post 1966). The blue-ribbon commission’s sanguine conclusions did not entirely allay the concerns of contemporary social critics.
In an open letter to President Johnson in 1966, the self-titled Ad Hoc Committee on The Triple Revolution, which included Nobel laureates Linus Pauling (chemistry) and Gunnar Myrdal (economics), as well as economic historian Robert Heilbroner, opined that “The traditional link between jobs and incomes is being broken. … The economy of abundance can sustain all citizens in comfort and economic security whether or not they engage in what is commonly reckoned as work” (quoted in Akst 2013).5 Writing separately in The Public Interest in 1965, Heilbroner argued that, “the new technology is threatening a whole new group of skills—the sorting, filing, checking, calculating, remembering, comparing, okaying skills—that are the special preserve of the office worker.
… In the end, as machines continue to invade society, duplicating greater and greater numbers of social tasks, it is human labor itself—at least, as we now think of ‘labor’—that is gradually rendered redundant” (pp. 33-36). In the five decades since the Ad Hoc Committee wrote its open letter, human labor has not been rendered redundant, as these scholars had feared.
But automation anxiety has clearly returned.